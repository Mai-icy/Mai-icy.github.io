<!DOCTYPE html>
<html>
  <head>
     
    <meta charset="UTF-8">
    <title>《图解机器学习算法》笔记——无监督学习2 - Mai Icy</title>
    <link rel="shortcut icon" href="/static/img/icon.png">
    <link rel="icon" href="/static/img/icon.png" sizes="192x192"/>
    
<link rel="stylesheet" href="/static/kico.css">
<link rel="stylesheet" href="/static/hingle.css">

    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <meta name="viewport" content="width=device-width, maximum-scale=1, initial-scale=1"/>
    <meta property="og:site_name" content="Mai Icy">
    <meta property="og:title" content="《图解机器学习算法》笔记——无监督学习2"/>
    
<meta name="generator" content="Hexo 6.2.0"></head>

  <body>
    <header>
    <div class="head-title">
        <h4>Mai Icy</h4>
    </div>
    <div class="head-action">
        <div class="toggle-btn"></div>
        <div class="light-btn"></div>
        <div class="search-btn"></div>
    </div>
    <form class="head-search" method="post">
        <input type="text" name="s" placeholder="搜索什么？">
    </form>
    <nav class="head-menu">
        <a href="/">首页</a>
        <div class="has-child">
            <a>分类</a>
            <div class="sub-menu">
                <a class="category-link" href="/categories/C/">C</a><a class="category-link" href="/categories/python/">python</a><a class="category-link" href="/categories/rust/">rust</a><a class="category-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a><a class="category-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="category-link" href="/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">算法学习笔记</a><a class="category-link" href="/categories/%E7%AE%97%E6%B3%95%E8%AF%BE%E7%AC%94%E8%AE%B0/">算法课笔记</a><a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/">计算机网络笔记</a>
            </div>
        </div>
        
            <a href="/about">关于我</a>
        
            <a href="/friends">朋友们</a>
        
    </nav>
</header>

    <main>
    <div class="wrap min">
        <section class="post-title">
            <h2>《图解机器学习算法》笔记——无监督学习2</h2>
            <div class="post-meta">
                <time class="date">2024.11.25</time>
            
                <span class="category"><a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span>
            
            </div>
        </section>
        <article class="post-content">
        
            <blockquote>这篇文章上次修改于 191 天前，可能其部分内容已经发生变化，如有疑问可询问作者。</blockquote>
        
            <h1>算法五：k-means算法</h1>
<h2 id="概述">概述</h2>
<ul>
<li>聚类：把相似的数据汇总为簇的方法</li>
</ul>
<p>该算法是一种聚类算法。</p>
<p>输入是多个数据点，并设置要聚类的簇数量，例如以下：</p>
<p><img src="/2024/11/25/machine-learning-note6/1.png" alt="1.png"></p>
<p>给定图a的数据点，要求分成3簇，图中给出了三个X，代表了三个簇的中心。求簇的重心就是聚类算法的重要计算。</p>
<h2 id="算法说明">算法说明</h2>
<p><img src="/2024/11/25/machine-learning-note6/2.png" alt="2.png"></p>
<p>如上图重复步骤：</p>
<ul>
<li>从数据点中随机选择和簇数量相同的点，作为簇的重心</li>
<li>计算每个数据点到各重心之间的距离，将自己归类为最近的簇重心的簇</li>
<li>计算每个簇的数据点的平均值，作为新的重心。</li>
<li>重复步骤2和步骤3.知道所有数据点不改变所属的簇。</li>
</ul>
<p>注：</p>
<p>有时选择的重心不好可能会导致步骤2和步骤3的训练无法顺利进行。</p>
<p>在随机选择的重心之间太近等的情况下，就会出现这种问题。</p>
<p>利用k-means++等方法，选择位置尽可能远离的数据点作为重心的初始值，就可以解决这个问题。</p>
<h2 id="示例代码">示例代码</h2>
<p>下面是对鸢尾花数据集应用k-means算法的代码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> KMeans
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris
data <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
n_clusters <span class="token operator">=</span> <span class="token number">3</span>  <span class="token comment"># 将簇的数量设置为3</span>
model <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span>n_clusters<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>data<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>labels_<span class="token punctuation">)</span>  <span class="token comment"># 各数据点所属的簇</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>cluster_centers_<span class="token punctuation">)</span>  <span class="token comment"># 通过fit()计算得到的重心</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="详细说明">详细说明</h2>
<h3 id="k-means的评估">k-means的评估</h3>
<p>用于评估分类的结果好坏，可以使用WCSS（簇内平方和）。也就是每个簇内的所有数据到重心距离的平方和相加的值。</p>
<p>WCSS越小，说明聚类结果越好。</p>
<h3 id="Elbow方法确定簇数量">Elbow方法确定簇数量</h3>
<p>对于簇数量，是作为输入的超参数，也就是我们算法名称中的k，有些数据集很难定义合适的簇数量。</p>
<p>我们已经知道可以使用WCSS来确定优劣，因此对于不同的簇数量，我们都可以得到一个WCSS。</p>
<p><img src="/2024/11/25/machine-learning-note6/3.png" alt="image.png"></p>
<p>纵轴是WCSS，横轴是k，可以发现，随着k的变大，WCSS会递减，但是我们可以找到一个点 3，在这个点后，WCSS的变化值显著减少，也就是选择这个点是最适合的点，这个图像类似于一个肘部，因此称作肘方法。</p>
<p>注：</p>
<p>当没有很明确的理由来确定簇的数量或对数据知之甚少时，Elbow方法很有用。不过在实际的分析中，常常不会出现图中那样明显的“肘部”，所以Elbow方法的结果只不过是一种参考而已。</p>
<h1>算法六：混合高斯分布</h1>
<h2 id="概述-2">概述</h2>
<p>如果数据集中有多组数据，可以使用混合高斯分布（即多个高斯分布的线性组合）来实现聚类。</p>
<p>高斯分布是一种概率分布的名称，使用概率分布产生的数据估计分布参数的方法。</p>
<p>高斯分布认为数据的分布可以用均值和方差表示（正态分布）</p>
<ul>
<li>均值：是数据的中心位置</li>
<li>方差：是数据的离散程度</li>
</ul>
<p>混合高斯分布是以多个高斯分布的线性叠加表示数据的模型。</p>
<p><img src="/2024/11/25/machine-learning-note6/4.png" alt="image.png"></p>
<p>以上是对鸢尾花数据使用高斯分布和混合高斯分布的结果。</p>
<p>对于高斯分布，每个轴只有一个均值和一个方差，所以使用一个分布无法提现不同品种的差异。</p>
<p>对于混合高斯分布，使用了三个高斯分布叠加而成，可以表示多个类别组成的复杂数据。</p>
<h2 id="算法说明-2">算法说明</h2>
<p>混合高斯分布的学习过程是从输入数据中找到每个高斯分布的均值和方差。</p>
<p>对于一个已经了解的高斯分布，做一维数据进行示例：</p>
<p><img src="/2024/11/25/machine-learning-note6/5.png" alt="image.png"></p>
<p>这个一维数据由两个高斯分布分类。</p>
<p>但实际上，混合高斯分布必须在不知道每个数据点的类别的情况下求出参数。</p>
<p>因此，我们需要在推测“每个数据点属于某个类别”的权重的基础上，计算出数据点的各个类别的高斯分布的参数（均值和方差）。</p>
<p>步骤：</p>
<ul>
<li>初始化参数（各高斯分布的均值和方差）</li>
<li>对每个类别计算数据点的权重</li>
<li>根据步骤2 中计算出的权重重新计算参数</li>
<li>重复 步骤2 步骤3 ，知道步骤3更新前后的每个均值的变化够小</li>
</ul>
<p><img src="/2024/11/25/machine-learning-note6/6.png" alt="image.png"></p>
<p>求解权重和参数通常使用期望最大化（Expectation-Maximization, EM）算法。在不知道数据点的类别标签的情况下，通过反复迭代估计权重和更新参数来逼近真实的高斯分布参数。</p>
<p>在步骤2中计算数据点拥有的权重，权重计算方法为“每个高斯分布的值/所有高斯分布的值之和“。当计算出所有数据点的权重后，求出每个类别的权重的均值，然后计算方差。</p>
<h2 id="示例代码-2">示例代码</h2>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>mixture <span class="token keyword">import</span> GaussianMixture
data <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
n_components <span class="token operator">=</span> <span class="token number">3</span>  <span class="token comment"># 高斯分布的数量</span>
model <span class="token operator">=</span> GaussianMixture<span class="token punctuation">(</span>n_components<span class="token operator">=</span>n_components<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>data<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>data<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 预测类别</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>means_<span class="token punctuation">)</span>  <span class="token comment"># 各高斯分布的均值</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>covariances_<span class="token punctuation">)</span>  <span class="token comment"># 各高斯分布的方差</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="详细说明-2">详细说明</h2>
<p>以下是对同一数据使用混合高斯分布和K-means进行聚类。</p>
<p><img src="/2024/11/25/machine-learning-note6/7.png" alt="image.png"></p>
<p>我们可以看到，混合高斯分布的数据会更优，因为高斯分布对于椭圆形数据更优，而k-means对重心开始呈圆形分布的数据会更好，因此椭圆成为了k-means的软肋，更该使用混合高斯分布。</p>
<h1>算法七：LLE</h1>
<h2 id="概述-3">概述</h2>
<p>无监督学习中的一项重要任务是将结构复杂的数据转化为更简单的形式。</p>
<p>LLE（Locally Linear Embedding，局部线性嵌入）可以将以弯曲或扭曲的状态埋藏在高维空间中的结构简单地表示在低维空间中。</p>
<p>LLE被称为流形学习，它的目标是对具有非线性结构的数据进行降维。</p>
<p>对于原始数据，是典型的瑞士卷数据集图像。LLE和PCA的降维结果如下所示。PCA更像简单的压扁，由于PCA更适合变量之间有一定关联性的数据，所以在降维时，LLE更适合。</p>
<p><img src="/2024/11/25/machine-learning-note6/8.png" alt="image.png"></p>
<h2 id="算法说明-3">算法说明</h2>
<p>LLE 是以降维后依旧保持原始高维空间中的局部线性组合关系作为核心</p>
<p>LLE算法要求数据点由其近邻点的线性组合来表示</p>
<p>对于数据点 x1，以最接近 x1 的两个点 x2 和 x3 的线性组合来表示它。</p>
<p><img src="https://math.now.sh?inline=x_1%3Dw_%7B12%7D%20x_2%20%2B%20w_%7B13%7D%20x_3" style="filter: opacity(90%);transform:scale(0.85);text-align:center;display:inline-block;margin: 0;"></p>
<p>可以得到两个权值w12，w13.</p>
<p>目的是通过降维后，依旧保持这个线性组合关系，如下图所示</p>
<p>LLE将高维的曲折关系转化为邻近点的组合关系。</p>
<p><img src="/2024/11/25/machine-learning-note6/9.png" alt="image.png"></p>
<p>对于近邻点数量，是超参数，设定为k个。</p>
<p>步骤：</p>
<ul>
<li>找到数据点 xi 的 k 个近邻点</li>
<li>求出由 k 个近邻点线性组合的权值</li>
<li>使用权值计算出低维度的 y_i</li>
</ul>
<p>在确定近邻点数量后，首先，为了求出权重 wij，我们将 xi 和 其近邻点的线性组合 的误差表示为</p>
<p><img src="https://math.now.sh?inline=x_i%20-%20%5Csum_j%20w_%7Bij%7D%20x_j" style="filter: opacity(90%);transform:scale(0.85);text-align:center;display:inline-block;margin: 0;"></p>
<p>随着 wij 值的变化，这个误差会增大或减少。</p>
<p>通过计算所有  xi 和线性组合的差的平方和，我们将权重 wij  与误差之间的关系表示为以下误差函数： <img src="https://math.now.sh?inline=e%28W%29%20%3D%20%5Csum_i%20%5Cleft%7C%20x_i%20-%20%5Csum_j%20w_%7Bij%7D%20x_j%20%5Cright%7C%5E2" style="filter: opacity(90%);transform:scale(0.85);text-align:center;display:inline-block;margin: 0;"></p>
<p>除了 wij 近邻点之外，表达式中的值都是 0。另外还有一个约束条件：对于某个 i，有</p>
<p><img src="https://math.now.sh?inline=%5Csum_j%20w_%7Bij%7D%20%3D%201" style="filter: opacity(90%);transform:scale(0.85);text-align:center;display:inline-block;margin: 0;"></p>
<p>我们可以认为权重 wij 表示数据点 xi 及其近邻点之间的关系，而 LLE 通过这种关系在低维空间中也得以保持。在计算完权重后，我们计算低维空间中表示数据点的 y_i：</p>
<p><img src="https://math.now.sh?inline=%5CPhi%28y%29%20%3D%20%5Csum_i%20%5Cleft%7C%20y_i%20-%20%5Csum_j%20w_%7Bij%7D%20y_j%20%5Cright%7C%5E2" style="filter: opacity(90%);transform:scale(0.85);text-align:center;display:inline-block;margin: 0;"></p>
<p>前面求出使误差最小的权重 wij，现在要做的是利用刚刚求出的 wij，求使误差最小的 y</p>
<h2 id="示例代码-3">示例代码</h2>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> samples_generator
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>manifold <span class="token keyword">import</span> LocallyLinearEmbedding
data<span class="token punctuation">,</span> color <span class="token operator">=</span> samples_generator<span class="token punctuation">.</span>make_swiss_roll<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">1500</span><span class="token punctuation">)</span>
n_neighbors <span class="token operator">=</span> <span class="token number">12</span>  <span class="token comment"># 近邻点的数量</span>
n_components <span class="token operator">=</span> <span class="token number">2</span>  <span class="token comment"># 降维后的维度</span>
model <span class="token operator">=</span> LocallyLinearEmbedding<span class="token punctuation">(</span>n_neighbors<span class="token operator">=</span>n_neighbors<span class="token punctuation">,</span>
n_components<span class="token operator">=</span>n_components<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 变换后的数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="详细说明-3">详细说明</h2>
<h3 id="流形学习">流形学习</h3>
<p>流形指的是可以将局部看做没有弯曲的空间，类似于将地球的局部绘制平面地图</p>
<p>“所谓的流形就是从局部来看是低维空间的结构被埋藏在高维空间里”</p>
<h3 id="近邻点数量">近邻点数量</h3>
<p>近邻点数量是超参数，对于不同的数量有不同的降维结果。如下图</p>
<p><img src="/2024/11/25/machine-learning-note6/10.png" alt="image.png"></p>
<p>当近邻点设置为5时，LLT没有连贯的结构，分布狭窄，反应信息少。</p>
<p>当近邻点设置为50时，不同颜色的点距离很近，无法把握局部结构。</p>
<p>这个参数十分重要，影响很大。</p>
<h1>算法八：t-SNE</h1>
<p>是一种将高维的复杂数据降维为二维（或三维）的算法，用于低维空间的可视化。</p>
<h2 id="概述-4">概述</h2>
<p>t-SNE是一种流形学习算法，用于可视化复杂数据。如下例子是将三维空间上的两个瑞士卷数据转变为二维空间的示例。</p>
<p><img src="/2024/11/25/machine-learning-note6/11.png" alt="image.png"></p>
<p>在上图体现出，t-SNE可以在低维空间展现多个结构。</p>
<h2 id="算法说明-4">算法说明</h2>
<p>步骤：</p>
<ol>
<li>对于所有的组 i、j，使用高斯分布来表示 xi 和 xj 的相似度</li>
<li>在低维空间中随机配置与 x i 相同数量的点 y i，对于所有的组 i、j，使用 t 分布表示 y i 和 y j 的相似度。</li>
<li>更新数据点 yi，使得步骤 1 和步骤 2 中定义的相似度分布尽可能相似。</li>
<li>重复步骤 3，直到达到收敛条件。</li>
</ol>
<p><img src="/2024/11/25/machine-learning-note6/12.png" alt="image.png"></p>
<p>相似度指的是 数据点之间的相似程度，使用上图的概率分布衡量。</p>
<p>横轴是距离，纵轴是相似度，越近越相似。</p>
<p>在降低维度的情况下，使他们的相似度保持不变，这就是t-SNE的降维逻辑，此时使用的是 t 分布。</p>
<p>为了在低维空间中更好地表现数据点之间的距离关系，t-SNE并未简单地在高维空间和低维空间中使用相同的概率分布，而是采用了不同的方式：在高维空间中使用<strong>高斯分布</strong>来计算数据点的相似度，而在低维空间中使用<strong>t分布</strong>来衡量相似度。</p>
<p>选择t分布的原因在于它的<strong>重尾特性</strong>，这使得在低维空间中远离某个聚类中心的点不容易被压缩到中心附近，从而保持了聚类之间的分离度。因此，在低维空间中，数据点的相似度分布可以更自然地反映出聚类结构，使得不同聚类之间的距离更加明显。</p>
<p>在这个过程中，t-SNE通过最小化<strong>KL散度</strong>（Kullback-Leibler散度）来度量高维和低维空间相似度分布之间的差异，从而在降维过程中尽量保持高维空间中的相似度关系。这种方法使得t-SNE能够在低维空间中还原数据的高维结构，同时使聚类和局部关系更为清晰。</p>
<p><img src="/2024/11/25/machine-learning-note6/13.png" alt="image.png"></p>
<p>如上为更新次数是250和500的情况。</p>
<p>t-SNE通常用于降维到三维或者二维，在高维空间中，远离重心的点占主导地位，局部信息无法保留，因此无法降维到四维或者更高空间。</p>
<h2 id="示例代码-4">示例代码</h2>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>manifold <span class="token keyword">import</span> TSNE
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_digits
data <span class="token operator">=</span> load_digits<span class="token punctuation">(</span><span class="token punctuation">)</span>
n_components <span class="token operator">=</span> <span class="token number">2</span>  <span class="token comment"># 设置降维后的维度为2</span>
model <span class="token operator">=</span> TSNE<span class="token punctuation">(</span>n_components<span class="token operator">=</span>n_components<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="详细说明-4">详细说明</h2>
<p>使用手写数组数据作为训练数据，对PCA，LLE，t-SNE三种方法进行降维，展现结果如下：</p>
<p><img src="/2024/11/25/machine-learning-note6/14.png" alt="image.png"></p>
<p>PCA虽然进行归类，但很混杂。</p>
<p>LLE适用于非线性数据，但如果数据不是像瑞士卷一样几种，就不能很好把握结构。</p>
<p>t-SNE 在二维空间进行了很好的分类。</p>

        </article>
        <section class="post-near">
            <ul>
                
                    <li>上一篇: <a href="/2024/11/26/database-huadb3/">数据库内核实验报告三：多版本并发控制</a></li>
                
                
                    <li>下一篇: <a href="/2024/11/21/machine-learning-note5/">《图解机器学习算法》笔记——无监督学习1</a></li>
                
            </ul>
        </section>
        
            <section class="post-tags">
            <a class="-none-link" href="/tags/LLE/" rel="tag">LLE</a><a class="-none-link" href="/tags/k-means/" rel="tag">k-means</a><a class="-none-link" href="/tags/t-SNE/" rel="tag">t-SNE</a><a class="-none-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><a class="-none-link" href="/tags/%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" rel="tag">混合高斯分布</a>
            </section>
        
    
        <section class="post-author">
        
            <figure class="author-avatar">
                <img src="https://avatars.githubusercontent.com/u/62082723" alt="Mai Icy" />
            </figure>
        
            <div class="author-info">
                <h4>Mai Icy</h4>
                <p>wwwwwww</p>
            </div>
        </section>
    
    </div>
</main>

    <footer>
    <div class="buttons">
        <a class="to-top" href="#"></a>
    </div>
    <div class="wrap min">
        <section class="widget">
            <div class="row">
                <div class="col-m-4">
                    <h3 class="title-recent">最新文章：</h3>
                    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/06/04/%E5%9F%BA%E4%BA%8EOceanbase%E7%AE%80%E5%8D%95%E6%9E%84%E5%BB%BARAG/">基于 Oceanbase 简单构建RAG</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/29/%E5%9C%A8%20surfacebook2%20%E4%B8%8A%E5%AE%89%E8%A3%85%20ubuntu%20%E5%8F%8C%E7%B3%BB%E7%BB%9F/">在 surfacebook2 上安装 ubuntu 双系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/21/%E4%BD%BF%E7%94%A8%20Bind%20%E6%90%AD%E5%BB%BA%20DNS%20%E6%9C%8D%E5%8A%A1%E5%99%A8/">使用 Bind 搭建 DNS 服务器</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/14/MCP%E5%8D%8F%E8%AE%AE%E4%BB%8B%E7%BB%8D/">MCP协议介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/07/Linux%20%E4%B8%8A%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6/">Linux 上的压缩文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/05/Docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%AD%A6%E4%B9%A0/">Docker基本使用学习</a></li></ul>
                </div>
                <div class="col-m-4">
                    <h3 class="title-date">时光机：</h3>
                    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li></ul>
                </div>
                <div class="col-m-4">
                    <h3 class="title-tags">标签云：</h3>
                    <a href="/tags/2-SAT/" style="font-size: 10px;">2-SAT</a> <a href="/tags/3D-3D-ICP/" style="font-size: 10px;">3D-3D:ICP</a> <a href="/tags/AC%E8%87%AA%E5%8A%A8%E6%9C%BA/" style="font-size: 10px;">AC自动机</a> <a href="/tags/ARISE%E4%BC%98%E5%8C%96/" style="font-size: 10px;">ARISE优化</a> <a href="/tags/B-%E6%A0%91/" style="font-size: 10px;">B+树</a> <a href="/tags/BitTorrent/" style="font-size: 10px;">BitTorrent</a> <a href="/tags/C/" style="font-size: 11.43px;">C</a> <a href="/tags/CDN/" style="font-size: 10px;">CDN</a> <a href="/tags/CDQ%E5%88%86%E6%B2%BB/" style="font-size: 10px;">CDQ分治</a> <a href="/tags/DNS/" style="font-size: 11.43px;">DNS</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/HTTP%E6%B5%81/" style="font-size: 10px;">HTTP流</a> <a href="/tags/KMP/" style="font-size: 10px;">KMP</a> <a href="/tags/KNN/" style="font-size: 10px;">KNN</a> <a href="/tags/LDA/" style="font-size: 10px;">LDA</a> <a href="/tags/LLE/" style="font-size: 10px;">LLE</a> <a href="/tags/LSA/" style="font-size: 10px;">LSA</a> <a href="/tags/LSM%E6%A0%91/" style="font-size: 10px;">LSM树</a> <a href="/tags/LightHouse/" style="font-size: 10px;">LightHouse</a> <a href="/tags/Linux/" style="font-size: 11.43px;">Linux</a> <a href="/tags/MCP/" style="font-size: 10px;">MCP</a> <a href="/tags/Manacher%E7%AE%97%E6%B3%95/" style="font-size: 10px;">Manacher算法</a> <a href="/tags/NMF/" style="font-size: 10px;">NMF</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/PCA/" style="font-size: 10px;">PCA</a> <a href="/tags/Qwen/" style="font-size: 10px;">Qwen</a> <a href="/tags/RAG/" style="font-size: 11.43px;">RAG</a> <a href="/tags/RDT/" style="font-size: 10px;">RDT</a> <a href="/tags/SLAM/" style="font-size: 10px;">SLAM</a> <a href="/tags/SMTP-POP3-IMAP/" style="font-size: 10px;">SMTP/POP3/IMAP</a> <a href="/tags/ST%E8%A1%A8/" style="font-size: 10px;">ST表</a> <a href="/tags/TCP/" style="font-size: 10px;">TCP</a> <a href="/tags/UDP/" style="font-size: 10px;">UDP</a> <a href="/tags/WebDAV/" style="font-size: 10px;">WebDAV</a> <a href="/tags/diesel/" style="font-size: 10px;">diesel</a> <a href="/tags/epoll/" style="font-size: 10px;">epoll</a> <a href="/tags/http/" style="font-size: 10px;">http</a> <a href="/tags/io%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/" style="font-size: 10px;">io多路复用</a> <a href="/tags/k-means/" style="font-size: 10px;">k-means</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/python/" style="font-size: 14.29px;">python</a> <a href="/tags/redo-undo%E6%97%A5%E5%BF%97/" style="font-size: 10px;">redo/undo日志</a> <a href="/tags/requests/" style="font-size: 11.43px;">requests</a> <a href="/tags/t-SNE/" style="font-size: 10px;">t-SNE</a> <a href="/tags/tarjan/" style="font-size: 10px;">tarjan</a> <a href="/tags/web%E7%BC%93%E5%AD%98/" style="font-size: 10px;">web缓存</a> <a href="/tags/%E4%B8%AD%E5%9B%BD%E5%89%A9%E4%BD%99%E5%AE%9A%E7%90%86/" style="font-size: 10px;">中国剩余定理</a> <a href="/tags/%E4%B9%90%E8%A7%82%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" style="font-size: 10px;">乐观并发控制</a> <a href="/tags/%E4%BA%8B%E5%8A%A1/" style="font-size: 10px;">事务</a> <a href="/tags/%E4%BA%8C%E5%88%86%E5%9B%BE%E5%8C%B9%E9%85%8D/" style="font-size: 10px;">二分图匹配</a> <a href="/tags/%E4%BA%8C%E5%88%86%E7%AD%94%E6%A1%88/" style="font-size: 10px;">二分答案</a> <a href="/tags/%E4%BD%8D%E5%9B%BE%E7%B4%A2%E5%BC%95/" style="font-size: 10px;">位图索引</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 11.43px;">动态规划</a> <a href="/tags/%E5%8D%8F%E8%AE%AE%E5%88%86%E5%B1%82/" style="font-size: 10px;">协议分层</a> <a href="/tags/%E5%8F%8C%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F/" style="font-size: 10px;">双连通分量</a> <a href="/tags/%E5%90%8C%E4%BD%99/" style="font-size: 10px;">同余</a> <a href="/tags/%E5%90%8C%E4%BD%99%E9%80%86%E5%85%83/" style="font-size: 10px;">同余逆元</a> <a href="/tags/%E5%90%8E%E7%BC%80SA/" style="font-size: 10px;">后缀SA</a> <a href="/tags/%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/" style="font-size: 10px;">哈希索引</a> <a href="/tags/%E5%9B%BE%E7%AE%97%E6%B3%95/" style="font-size: 14.29px;">图算法</a> <a href="/tags/%E5%9B%BE%E8%AE%BA/" style="font-size: 14.29px;">图论</a> <a href="/tags/%E5%9F%BA%E7%8E%AF%E6%A0%91/" style="font-size: 10px;">基环树</a> <a href="/tags/%E5%A4%9A%E7%89%88%E6%9C%AC%E6%9C%BA%E5%88%B6/" style="font-size: 10px;">多版本机制</a> <a href="/tags/%E5%A4%9A%E7%BB%B4%E7%B4%A2%E5%BC%95/" style="font-size: 10px;">多维索引</a> <a href="/tags/%E5%A4%9A%E8%B7%AF%E5%88%86%E8%A7%A3/" style="font-size: 10px;">多路分解</a> <a href="/tags/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/" style="font-size: 10px;">多路复用</a> <a href="/tags/%E5%AD%97%E5%85%B8%E6%A0%91/" style="font-size: 10px;">字典树</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 10px;">字符串</a> <a href="/tags/%E5%AE%B9%E6%96%A5%E5%8E%9F%E7%90%86/" style="font-size: 10px;">容斥原理</a> <a href="/tags/%E5%B7%AE%E5%88%86%E7%BA%A6%E6%9D%9F/" style="font-size: 10px;">差分约束</a> <a href="/tags/%E5%BA%B7%E6%89%98%E5%B1%95%E5%BC%80/" style="font-size: 10px;">康托展开</a> <a href="/tags/%E5%BC%82%E6%88%96%E5%93%88%E5%B8%8C/" style="font-size: 10px;">异或哈希</a> <a href="/tags/%E5%BC%82%E6%AD%A5/" style="font-size: 10px;">异步</a> <a href="/tags/%E5%BC%BA%E8%81%94%E9%80%9A%E5%88%86%E9%87%8F/" style="font-size: 11.43px;">强联通分量</a> <a href="/tags/%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2FFT/" style="font-size: 10px;">快速傅里叶变换FFT</a> <a href="/tags/%E5%BF%AB%E9%80%9F%E5%B9%82/" style="font-size: 10px;">快速幂</a> <a href="/tags/%E6%82%B2%E8%A7%82%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" style="font-size: 10px;">悲观并发控制</a> <a href="/tags/%E6%8E%A5%E5%85%A5%E6%8A%80%E6%9C%AF/" style="font-size: 10px;">接入技术</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" style="font-size: 10px;">支持向量机</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 17.14px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8/" style="font-size: 10px;">数据库存储</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E9%AA%8C/" style="font-size: 15.71px;">数据库实验</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%95%B0%E8%AE%BA/" style="font-size: 10px;">数论</a> <a href="/tags/%E6%97%B6%E9%97%B4%E6%88%B3%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6/" style="font-size: 10px;">时间戳排序机制</a> <a href="/tags/%E6%9C%80%E5%A4%A7%E6%B5%81/" style="font-size: 10px;">最大流</a> <a href="/tags/%E6%9C%80%E5%B0%8F%E5%89%B2/" style="font-size: 10px;">最小割</a> <a href="/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84/" style="font-size: 10px;">最短路径</a> <a href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">朴素贝叶斯</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 17.14px;">机器学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A6%81/" style="font-size: 10px;">机器学习概要</a> <a href="/tags/%E6%9E%81%E8%A7%92%E6%8E%92%E5%BA%8F/" style="font-size: 10px;">极角排序</a> <a href="/tags/%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/" style="font-size: 10px;">查询优化</a> <a href="/tags/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86/" style="font-size: 10px;">查询处理</a> <a href="/tags/%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C/" style="font-size: 10px;">查询执行</a> <a href="/tags/%E6%A0%91%E4%B8%8A%E5%90%AF%E5%8F%91%E5%BC%8F%E5%90%88%E5%B9%B6/" style="font-size: 10px;">树上启发式合并</a> <a href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/" style="font-size: 10px;">树状数组</a> <a href="/tags/%E6%A0%91%E9%93%BE%E5%89%96%E5%88%86/" style="font-size: 10px;">树链剖分</a> <a href="/tags/%E6%A0%B9%E5%8F%B7%E5%88%86%E6%B2%BB/" style="font-size: 10px;">根号分治</a> <a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" style="font-size: 10px;">正则化</a> <a href="/tags/%E6%AF%8D%E5%87%BD%E6%95%B0/" style="font-size: 10px;">母函数</a> <a href="/tags/%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" style="font-size: 10px;">混合高斯分布</a> <a href="/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/" style="font-size: 11.43px;">源码阅读</a> <a href="/tags/%E7%8A%B6%E6%80%81%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">状态压缩</a> <a href="/tags/%E7%94%B5%E5%AD%90%E9%82%AE%E4%BB%B6/" style="font-size: 10px;">电子邮件</a> <a href="/tags/%E7%9F%A9%E9%98%B5/" style="font-size: 10px;">矩阵</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">神经网络</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 20px;">算法</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/" style="font-size: 10px;">线段树</a> <a href="/tags/%E7%BB%84%E5%90%88%E5%8D%9A%E5%BC%88/" style="font-size: 10px;">组合博弈</a> <a href="/tags/%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">网络</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E6%B5%81/" style="font-size: 12.86px;">网络流</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" style="font-size: 10px;">网络爬虫</a> <a href="/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/" style="font-size: 10px;">背包问题</a> <a href="/tags/%E8%8E%AB%E6%AF%94%E4%B9%8C%E6%96%AF%E5%8F%8D%E6%BC%94/" style="font-size: 10px;">莫比乌斯反演</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95/" style="font-size: 10px;">计算几何</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 18.57px;">计算机网络</a> <a href="/tags/%E8%B4%AA%E5%BF%83/" style="font-size: 10px;">贪心</a> <a href="/tags/%E8%B4%B9%E7%94%A8%E6%B5%81/" style="font-size: 10px;">费用流</a> <a href="/tags/%E9%80%92%E6%8E%A8/" style="font-size: 10px;">递推</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">逻辑回归</a> <a href="/tags/%E9%94%81%E6%9C%BA%E5%88%B6/" style="font-size: 10px;">锁机制</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" style="font-size: 10px;">随机森林</a>
                </div>
            </div>
        </section>
        <section class="sub-footer">
            <p>© 2025 <a href="/">Mai Icy</a>. All Rights Reserved. Theme By <a href="https://github.com/Dreamer-Paul/Hingle" target="_blank" rel="nofollow">Hingle</a>.</p>
        </section>
    </div>
</footer>


<script src="/static/kico.js"></script>
<script src="/static/hingle.js"></script>


<script>var hingle = new Paul_Hingle({"copyright":true,"night":true});</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
